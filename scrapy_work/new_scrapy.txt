安装scrapy框架
1.安装scrapy：通过pip install scrapy安装
2.如果在Windows下，还需要安装pypwin32，通过pip install pypwin32

创建爬虫：
创建项目："scrapy startproject[爬虫名字]"
创建爬虫：进入所在项目，执行命令："scrapy genspider[爬虫名字][爬虫的域名]"

项目结构：

1.items.py:用来存放爬虫爬取下来数据的模型
2.middlewares.py：用来存放各种中间文件
3.pipelines.py：用来将items的模型存储到本地磁盘
4、seetings.py：本爬虫的配置信息（比如请求头、多久发一次请求、ip代理池）
5.scrapy.cfg：项目的配置文件
6.spiders包：以后的爬虫都存放在这个里面


爬取及运行方法：
1.修改settings,里面的，ROBOTSTXT_OBEY改为false。修改请求头DEFAULT_REQUEST_HEADERS。
2.在spider里面写主爬虫程序。在parse里写爬虫程序。
3.通过scrapy crawl qsbk_spider运行程序。
4.也可在文件根目录建一个start文件通过命令行运行，代码如下
from scrapy import cmdline

cmdline.execute(['scrapy','crawl','qsbk_spider'])

糗事百科scrapy笔记
1.response可执行xpath和css语法提取数据
2.提取出来的数据是一个selector或者是一个selectorList对象，如果想获取其中的字符串，需要执行getall或者get。
3.getall：获取selector中所有文本，返回一个列表。
4.get方法：获取的是selector中第一个文本，返回的是str
5.如果数据请求解析回来，要传给pipline除了，可以使用yield来返回。或者收集所有的item，最后统一使用return返回，
6.item：建议在items.py中定义好模型，以后就不要使用字典
7.pipeline：这个是专门用来保存数据的，其中
	open_spider(self,spider)当爬虫打开的时候执行
	process_item（self,item,spider）当爬虫有item传过来时会被调用
	close_sider(self,spider):当爬虫关闭时会被调用

8.要激活pipline，在settings.py里面，设置TIME_PIPLINES
